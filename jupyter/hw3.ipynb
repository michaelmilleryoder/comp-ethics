{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import f1_score, make_scorer, precision_score, recall_score, accuracy_score\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import TweetTokenizer, pos_tag\n",
    "from IPython.display import display\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Embedding, Flatten, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hate</th>\n",
       "      <th>offensive</th>\n",
       "      <th>neither</th>\n",
       "      <th>tweet_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@HarveyLevinTMZ @sly309 they want what I got, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; &lt;mention&gt; they want what i got , i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@HeauxmerSimpson I'm jus tryna vaca away from ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; i'm jus tryna vaca away from the nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@HeilSidious Sion is a faggot. Very meh ganks ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; sion is a faggot . very meh ganks pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@Herfarm @SteveWorks4You @TheDemocrats your lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; &lt;mention&gt; &lt;mention&gt; your low info re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@Herfarm Move out of our country teabaggers!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; move out of our country teabaggers !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>@HollowDaDonLOM those god damn chinks. http://...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; those god damn chinks . http://t.co/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>@Hovaa_ #niggerfood #niggerperson #you #are #a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; #niggerfood #niggerperson #you #are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>@Hovaa_ ok wat ever u say whitey. u prolly nev...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; ok wat ever u say whitey . u prolly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>@Hovaa_ shut up lizard faggot nigger cunt</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; shut up lizard faggot nigger cunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>@Hovaa_ ya I know all the slang I'm racist I h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; ya i know all the slang i'm racist i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>@HuffingtonPost if the faggots can have a page...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; if the faggots can have a page y can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>@HuffingtonPost im American, don't give 2 shit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; im american , don't give 2 shits abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>@HuffingtonPost y don't that bitch just shoot ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; y don't that bitch just shoot hersel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>@Hunglikerobby_ that was so gay. And I was tan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; that was so gay . and i was tanning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>@Huntermoore happy cuz I woke up white and not...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; happy cuz i woke up white and not a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>@Huntermoore nigger!!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; nigger ! !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>@IDisDummies Can you speak one coherent senten...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; can you speak one coherent sentence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>@IGGYAZALEA it was a joke you fugly bitch be h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; it was a joke you fugly bitch be hap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>@IWantHis_Heart Ok u fat precious lookin ass b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; ok u fat precious lookin ass bitch ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>@IamShady_ that's not even a real word! Must h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; that's not even a real word ! must h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>@Iemonaids done that spell check we know you m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; done that spell check we know you me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>@ImToBlame you a fatherless wallet carrying as...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; you a fatherless wallet carrying ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>@Im_Thirst randies are fags</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; randies are fags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>@ImpactParasite just fuckin roasted this faggot</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; just fuckin roasted this faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>@InTheOvenJews @SlaveCatcher88 @marylene58 we ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; &lt;mention&gt; &lt;mention&gt; we should hang o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>@InfidelAlie that should read suck on my bacon...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; that should read suck on my bacon en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>@ItsKeeshKapeesh *yourself *faggot buy a dicti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; * yourself * faggot buy a dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>@ItsMander @kfc\\nYou can always gets da colore...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; &lt;mention&gt; you can always gets da col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>@ItsYOUR_man nothin much cousin touching white...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; nothin much cousin touching white tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>@JGardenofEden @KySportsRadio @ryanlemond But....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;mention&gt; &lt;mention&gt; &lt;mention&gt; but ... country ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21053</th>\n",
       "      <td>21053</td>\n",
       "      <td>you ain't gotta be a dyke to like hoes</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you ain't gotta be a dyke to like hoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21054</th>\n",
       "      <td>21054</td>\n",
       "      <td>you are a hoe, hoe, &amp;amp; a hoe.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you are a hoe , hoe , &amp; a hoe .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21055</th>\n",
       "      <td>21055</td>\n",
       "      <td>you bitches love yall some corny nigga</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you bitches love yall some corny nigga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21056</th>\n",
       "      <td>21056</td>\n",
       "      <td>you can masturbate anytime bitch lol &amp;#8220;@g...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you can masturbate anytime bitch lol ‚Äú &lt;mentio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21057</th>\n",
       "      <td>21057</td>\n",
       "      <td>you can never get a group of hoes together wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you can never get a group of hoes together wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21058</th>\n",
       "      <td>21058</td>\n",
       "      <td>you can tell when dick recently been in a puss...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you can tell when dick recently been in a puss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21059</th>\n",
       "      <td>21059</td>\n",
       "      <td>you can't cuff a hoe lmao</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you can't cuff a hoe lmao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21060</th>\n",
       "      <td>21060</td>\n",
       "      <td>you drove me redneck crazy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you drove me redneck crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21061</th>\n",
       "      <td>21061</td>\n",
       "      <td>you fake niggah lolol</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you fake niggah lolol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21062</th>\n",
       "      <td>21062</td>\n",
       "      <td>you got niggas, and i got bitches.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you got niggas , and i got bitches .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21063</th>\n",
       "      <td>21063</td>\n",
       "      <td>you gotta be a new breed of retarded if you do...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you gotta be a new breed of retarded if you do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21064</th>\n",
       "      <td>21064</td>\n",
       "      <td>you gotta understand that these bitches are ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you gotta understand that these bitches are ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21065</th>\n",
       "      <td>21065</td>\n",
       "      <td>you hoe spice</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you hoe spice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21066</th>\n",
       "      <td>21066</td>\n",
       "      <td>you just want some attention hoe</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you just want some attention hoe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21067</th>\n",
       "      <td>21067</td>\n",
       "      <td>you know what they say, the early bird gets th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>you know what they say , the early bird gets t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21068</th>\n",
       "      <td>21068</td>\n",
       "      <td>you know what your doing when you favorite a t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you know what your doing when you favorite a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21069</th>\n",
       "      <td>21069</td>\n",
       "      <td>you lil dumb ass bitch, i ain't fuckin wit chu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you lil dumb ass bitch , i ain't fuckin wit ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21070</th>\n",
       "      <td>21070</td>\n",
       "      <td>you look like AC Green...bitch don't call here...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you look like ac green ... bitch don't call he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21071</th>\n",
       "      <td>21071</td>\n",
       "      <td>you look like your 12 stop talking about fucki...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you look like your 12 stop talking about fucki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21072</th>\n",
       "      <td>21072</td>\n",
       "      <td>you might as well gone pussy pop on a stage</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you might as well gone pussy pop on a stage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21073</th>\n",
       "      <td>21073</td>\n",
       "      <td>you niggers cheat on ya gf's? smh....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you niggers cheat on ya gf's ? smh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21074</th>\n",
       "      <td>21074</td>\n",
       "      <td>you really care bout dis bitch. my dick all in...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you really care bout dis bitch . my dick all i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21075</th>\n",
       "      <td>21075</td>\n",
       "      <td>you worried bout other bitches, you need me for?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you worried bout other bitches , you need me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21076</th>\n",
       "      <td>21076</td>\n",
       "      <td>you're all niggers</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you're all niggers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21077</th>\n",
       "      <td>21077</td>\n",
       "      <td>you're such a retard i hope you get type 2 dia...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you're such a retard i hope you get type 2 dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21078</th>\n",
       "      <td>21078</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you's a muthaf * * * in lie ‚Äú &lt;mention&gt; : &lt;men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21079</th>\n",
       "      <td>21079</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>you've gone and broke the wrong heart baby , a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21080</th>\n",
       "      <td>21080</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>young buck wanna eat ! ! .. dat nigguh like i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21081</th>\n",
       "      <td>21081</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21082</th>\n",
       "      <td>21082</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>~ ~ ruffled | ntac eileen dahlia - beautiful c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21083 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              tweet  hate  \\\n",
       "0               0  @HarveyLevinTMZ @sly309 they want what I got, ...     1   \n",
       "1               1  @HeauxmerSimpson I'm jus tryna vaca away from ...     1   \n",
       "2               2  @HeilSidious Sion is a faggot. Very meh ganks ...     1   \n",
       "3               3  @Herfarm @SteveWorks4You @TheDemocrats your lo...     1   \n",
       "4               4       @Herfarm Move out of our country teabaggers!     1   \n",
       "5               5  @HollowDaDonLOM those god damn chinks. http://...     1   \n",
       "6               6  @Hovaa_ #niggerfood #niggerperson #you #are #a...     1   \n",
       "7               7  @Hovaa_ ok wat ever u say whitey. u prolly nev...     1   \n",
       "8               8          @Hovaa_ shut up lizard faggot nigger cunt     1   \n",
       "9               9  @Hovaa_ ya I know all the slang I'm racist I h...     1   \n",
       "10             10  @HuffingtonPost if the faggots can have a page...     1   \n",
       "11             11  @HuffingtonPost im American, don't give 2 shit...     1   \n",
       "12             12  @HuffingtonPost y don't that bitch just shoot ...     1   \n",
       "13             13  @Hunglikerobby_ that was so gay. And I was tan...     1   \n",
       "14             14  @Huntermoore happy cuz I woke up white and not...     1   \n",
       "15             15                              @Huntermoore nigger!!     1   \n",
       "16             16  @IDisDummies Can you speak one coherent senten...     1   \n",
       "17             17  @IGGYAZALEA it was a joke you fugly bitch be h...     1   \n",
       "18             18  @IWantHis_Heart Ok u fat precious lookin ass b...     1   \n",
       "19             19  @IamShady_ that's not even a real word! Must h...     1   \n",
       "20             20  @Iemonaids done that spell check we know you m...     1   \n",
       "21             21  @ImToBlame you a fatherless wallet carrying as...     1   \n",
       "22             22                        @Im_Thirst randies are fags     1   \n",
       "23             23    @ImpactParasite just fuckin roasted this faggot     1   \n",
       "24             24  @InTheOvenJews @SlaveCatcher88 @marylene58 we ...     1   \n",
       "25             25  @InfidelAlie that should read suck on my bacon...     1   \n",
       "26             26  @ItsKeeshKapeesh *yourself *faggot buy a dicti...     1   \n",
       "27             27  @ItsMander @kfc\\nYou can always gets da colore...     1   \n",
       "28             28  @ItsYOUR_man nothin much cousin touching white...     1   \n",
       "29             29  @JGardenofEden @KySportsRadio @ryanlemond But....     1   \n",
       "...           ...                                                ...   ...   \n",
       "21053       21053             you ain't gotta be a dyke to like hoes     0   \n",
       "21054       21054                   you are a hoe, hoe, &amp; a hoe.     0   \n",
       "21055       21055             you bitches love yall some corny nigga     0   \n",
       "21056       21056  you can masturbate anytime bitch lol &#8220;@g...     0   \n",
       "21057       21057  you can never get a group of hoes together wit...     0   \n",
       "21058       21058  you can tell when dick recently been in a puss...     0   \n",
       "21059       21059                          you can't cuff a hoe lmao     0   \n",
       "21060       21060                         you drove me redneck crazy     0   \n",
       "21061       21061                              you fake niggah lolol     0   \n",
       "21062       21062                 you got niggas, and i got bitches.     0   \n",
       "21063       21063  you gotta be a new breed of retarded if you do...     0   \n",
       "21064       21064  you gotta understand that these bitches are ch...     0   \n",
       "21065       21065                                      you hoe spice     0   \n",
       "21066       21066                   you just want some attention hoe     0   \n",
       "21067       21067  you know what they say, the early bird gets th...     0   \n",
       "21068       21068  you know what your doing when you favorite a t...     0   \n",
       "21069       21069  you lil dumb ass bitch, i ain't fuckin wit chu...     0   \n",
       "21070       21070  you look like AC Green...bitch don't call here...     0   \n",
       "21071       21071  you look like your 12 stop talking about fucki...     0   \n",
       "21072       21072        you might as well gone pussy pop on a stage     0   \n",
       "21073       21073              you niggers cheat on ya gf's? smh....     0   \n",
       "21074       21074  you really care bout dis bitch. my dick all in...     0   \n",
       "21075       21075   you worried bout other bitches, you need me for?     0   \n",
       "21076       21076                                 you're all niggers     1   \n",
       "21077       21077  you're such a retard i hope you get type 2 dia...     1   \n",
       "21078       21078  you's a muthaf***in lie &#8220;@LifeAsKing: @2...     0   \n",
       "21079       21079  you've gone and broke the wrong heart baby, an...     0   \n",
       "21080       21080  young buck wanna eat!!.. dat nigguh like I ain...     0   \n",
       "21081       21081              youu got wild bitches tellin you lies     0   \n",
       "21082       21082  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...     0   \n",
       "\n",
       "       offensive  neither                                    tweet_processed  \n",
       "0              0        0  <mention> <mention> they want what i got , i w...  \n",
       "1              0        0  <mention> i'm jus tryna vaca away from the nig...  \n",
       "2              0        0  <mention> sion is a faggot . very meh ganks pr...  \n",
       "3              0        0  <mention> <mention> <mention> your low info re...  \n",
       "4              0        0     <mention> move out of our country teabaggers !  \n",
       "5              0        0  <mention> those god damn chinks . http://t.co/...  \n",
       "6              0        0  <mention> #niggerfood #niggerperson #you #are ...  \n",
       "7              0        0  <mention> ok wat ever u say whitey . u prolly ...  \n",
       "8              0        0        <mention> shut up lizard faggot nigger cunt  \n",
       "9              0        0  <mention> ya i know all the slang i'm racist i...  \n",
       "10             0        0  <mention> if the faggots can have a page y can...  \n",
       "11             0        0  <mention> im american , don't give 2 shits abo...  \n",
       "12             0        0  <mention> y don't that bitch just shoot hersel...  \n",
       "13             0        0  <mention> that was so gay . and i was tanning ...  \n",
       "14             0        0  <mention> happy cuz i woke up white and not a ...  \n",
       "15             0        0                               <mention> nigger ! !  \n",
       "16             0        0  <mention> can you speak one coherent sentence ...  \n",
       "17             0        0  <mention> it was a joke you fugly bitch be hap...  \n",
       "18             0        0  <mention> ok u fat precious lookin ass bitch ,...  \n",
       "19             0        0  <mention> that's not even a real word ! must h...  \n",
       "20             0        0  <mention> done that spell check we know you me...  \n",
       "21             0        0  <mention> you a fatherless wallet carrying ass...  \n",
       "22             0        0                         <mention> randies are fags  \n",
       "23             0        0          <mention> just fuckin roasted this faggot  \n",
       "24             0        0  <mention> <mention> <mention> we should hang o...  \n",
       "25             0        0  <mention> that should read suck on my bacon en...  \n",
       "26             0        0     <mention> * yourself * faggot buy a dictionary  \n",
       "27             0        0  <mention> <mention> you can always gets da col...  \n",
       "28             0        0  <mention> nothin much cousin touching white tr...  \n",
       "29             0        0  <mention> <mention> <mention> but ... country ...  \n",
       "...          ...      ...                                                ...  \n",
       "21053          1        0             you ain't gotta be a dyke to like hoes  \n",
       "21054          1        0                    you are a hoe , hoe , & a hoe .  \n",
       "21055          1        0             you bitches love yall some corny nigga  \n",
       "21056          1        0  you can masturbate anytime bitch lol ‚Äú <mentio...  \n",
       "21057          1        0  you can never get a group of hoes together wit...  \n",
       "21058          1        0  you can tell when dick recently been in a puss...  \n",
       "21059          1        0                          you can't cuff a hoe lmao  \n",
       "21060          1        0                         you drove me redneck crazy  \n",
       "21061          1        0                              you fake niggah lolol  \n",
       "21062          1        0               you got niggas , and i got bitches .  \n",
       "21063          1        0  you gotta be a new breed of retarded if you do...  \n",
       "21064          1        0  you gotta understand that these bitches are ch...  \n",
       "21065          1        0                                      you hoe spice  \n",
       "21066          1        0                   you just want some attention hoe  \n",
       "21067          0        1  you know what they say , the early bird gets t...  \n",
       "21068          1        0  you know what your doing when you favorite a t...  \n",
       "21069          1        0  you lil dumb ass bitch , i ain't fuckin wit ch...  \n",
       "21070          1        0  you look like ac green ... bitch don't call he...  \n",
       "21071          1        0  you look like your 12 stop talking about fucki...  \n",
       "21072          1        0        you might as well gone pussy pop on a stage  \n",
       "21073          1        0             you niggers cheat on ya gf's ? smh ...  \n",
       "21074          1        0  you really care bout dis bitch . my dick all i...  \n",
       "21075          1        0  you worried bout other bitches , you need me f...  \n",
       "21076          0        0                                 you're all niggers  \n",
       "21077          0        0  you're such a retard i hope you get type 2 dia...  \n",
       "21078          1        0  you's a muthaf * * * in lie ‚Äú <mention> : <men...  \n",
       "21079          0        1  you've gone and broke the wrong heart baby , a...  \n",
       "21080          1        0  young buck wanna eat ! ! .. dat nigguh like i ...  \n",
       "21081          1        0              youu got wild bitches tellin you lies  \n",
       "21082          0        1  ~ ~ ruffled | ntac eileen dahlia - beautiful c...  \n",
       "\n",
       "[21083 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = {}\n",
    "# data['train'] = pd.read_csv('../../hw3/train_mod.csv')\n",
    "data['train'] = pd.read_csv('/usr0/home/mamille2/11-830_data/hw3/train_mod.csv')\n",
    "data['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_tweet(text):\n",
    "    \n",
    "    # Remove @mentions\n",
    "    p = re.sub(r'@\\w+', '<mention>', text)\n",
    "    \n",
    "    # tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    p = ' '.join(tokenizer.tokenize(p.lower()))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['train']['tweet_processed'] = data['train']['tweet'].map(lambda x: preprocess_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <mention> <mention> they want what i got , i want what they got , america.they got the word nigger , i want it to                           \n",
       "1    <mention> i'm jus tryna vaca away from the niggers bro .                                                                                    \n",
       "2    <mention> sion is a faggot . very meh ganks pre - 6 and post - 6 his ganks are still pretty meh he's just a tank , nothing special imo mao +\n",
       "3    <mention> <mention> <mention> your low info redneck dumbasses vote for these corporate shills that idiots like you believe #dummies         \n",
       "4    <mention> move out of our country teabaggers !                                                                                              \n",
       "Name: tweet_processed, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['tweet_processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['train'].to_csv('../../hw3/train_mod.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 8534)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(min_df=2)\n",
    "vec.fit(data['train'].tweet_processed.tolist())\n",
    "\n",
    "bow = {}\n",
    "bow['train'] = vec.transform(data['train'].tweet_processed.tolist())\n",
    "bow['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.8%\n",
      "Precision:  48.5%\n",
      "Recall:  21.3%\n",
      "F1:  25.7%\n"
     ]
    }
   ],
   "source": [
    "# Hate speech classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = bow['train']\n",
    "y['train'] = data['train'].hate\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  89.2%\n",
      "Precision:  92.8%\n",
      "Recall:  93.3%\n",
      "F1:  93.0%\n"
     ]
    }
   ],
   "source": [
    "# Offensive language classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = bow['train']\n",
    "y['train'] = data['train'].offensive\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis (hate speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on full dataset (hate speech) and see errors\n",
    "clf = LogisticRegression()\n",
    "y['train'] = data['train'].hate\n",
    "clf.fit(x['train'], y['train'])\n",
    "preds = clf.predict(x['train'])\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "695\n"
     ]
    }
   ],
   "source": [
    "false_pos = []\n",
    "false_neg = []\n",
    "\n",
    "for i, (gold, pred) in enumerate(zip(data['train'].hate.tolist(), preds)):\n",
    "    if pred != gold:\n",
    "        if gold == 0:\n",
    "            false_pos.append(i)\n",
    "        elif gold == 1:\n",
    "            false_neg.append(i)\n",
    "        else:\n",
    "            \"What???\"\n",
    "            \n",
    "print(len(false_pos))\n",
    "print(len(false_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negatives:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_processed</th>\n",
       "      <th>hate</th>\n",
       "      <th>offensive</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19933</th>\n",
       "      <td>bitch kill yoself , go on to the bathroom and eat the pills bitch , all of em</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10717</th>\n",
       "      <td>rt &lt;mention&gt; : &lt;mention&gt; &lt;mention&gt; &lt;mention&gt; not about to shoot this pussy ! ima beat his face into the asphalt t ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12006</th>\n",
       "      <td>rt &lt;mention&gt; : the fox says you're a cunt .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>&lt;mention&gt; fucking coon</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>&lt;mention&gt; &lt;mention&gt; &lt;mention&gt; &lt;mention&gt; yeah ok nigger .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13655</th>\n",
       "      <td>rt &lt;mention&gt; : &lt;mention&gt; happy birthday my little cotton picker</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>rt &lt;mention&gt; : i saw a great chimpout 2 summers ago . a middle aged salvadoran and his 15 yr old son got wasted by 6 beaners at the metlife ‚Ä¶</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>&lt;mention&gt; faggot</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>&lt;mention&gt; i'm a faggot :/ http://t.co/gr627ldfaa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4865</th>\n",
       "      <td>curse avi for coke because he's a fag and hasn't put it on yet http://t.co/jhqpwop4wk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16574</th>\n",
       "      <td>rt &lt;mention&gt; : if i was obama id call a press conference &amp; slit joe bidens neck on live tv just 2 show these crackers i mean business ‚Ä¶</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11945</th>\n",
       "      <td>rt &lt;mention&gt; : pillow talking too yo hoe hating on a nigga fuck around loose ya hoe like that üíØ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>&lt;mention&gt; \" nigger \"</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19811</th>\n",
       "      <td>youre a fucking ugly bitch , i wanna stab you to death and then play around with your blood - patrick bateman</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18137</th>\n",
       "      <td>these niggas retarded ... lmfaoo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4526</th>\n",
       "      <td>bullets through a nigguh tru jeans</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16180</th>\n",
       "      <td>rt &lt;mention&gt; : who wants to chill on campus w me &amp; throw bibles at bitches taking the walk of shame ‚ùì ‚ùî</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>&lt;mention&gt; ur ballsack is hairy gtfo tranny prude bitch</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>&lt;mention&gt; im going to murder that fucking cunt</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19825</th>\n",
       "      <td>zimmerman we comin for yo life bitch . http://t.co/khdz1vdnsk</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>&lt;mention&gt; your 2 . and 3 . nigga them hoes suck wayans brothers are not funny at all jamie foxx should be number 2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12308</th>\n",
       "      <td>rt &lt;mention&gt; : nigga wanna bitch about a party y'all know my address knock at my door</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>rt &lt;mention&gt; : i hate trailer trash üò∑</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>ayy carlton is a ghetto school now #switchedatbirth</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16770</th>\n",
       "      <td>raunchy ass hoe .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10664</th>\n",
       "      <td>rt &lt;mention&gt; : anybody who trashes their own neighborhood after a perceived injustice is a fucking moron . #ferguson</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>&lt;mention&gt; i don't follow you because everything about your company is white trash .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14976</th>\n",
       "      <td>rt &lt;mention&gt; : in the future , gay culture will be considered cool by america's teens and instead of wiggers there will be straggots .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20223</th>\n",
       "      <td>hoe ass nigga üòπ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>&lt;mention&gt; stop trolling me u creepy ass cracker ! ! !</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     tweet_processed  \\\n",
       "19933  bitch kill yoself , go on to the bathroom and eat the pills bitch , all of em                                                                   \n",
       "10717  rt <mention> : <mention> <mention> <mention> not about to shoot this pussy ! ima beat his face into the asphalt t ...                           \n",
       "12006  rt <mention> : the fox says you're a cunt .                                                                                                     \n",
       "738    <mention> fucking coon                                                                                                                          \n",
       "3340   <mention> <mention> <mention> <mention> yeah ok nigger .                                                                                        \n",
       "13655  rt <mention> : <mention> happy birthday my little cotton picker                                                                                 \n",
       "14387  rt <mention> : i saw a great chimpout 2 summers ago . a middle aged salvadoran and his 15 yr old son got wasted by 6 beaners at the metlife ‚Ä¶   \n",
       "2811   <mention> faggot                                                                                                                                \n",
       "3255   <mention> i'm a faggot :/ http://t.co/gr627ldfaa                                                                                                \n",
       "4865   curse avi for coke because he's a fag and hasn't put it on yet http://t.co/jhqpwop4wk                                                           \n",
       "16574  rt <mention> : if i was obama id call a press conference & slit joe bidens neck on live tv just 2 show these crackers i mean business ‚Ä¶         \n",
       "11945  rt <mention> : pillow talking too yo hoe hating on a nigga fuck around loose ya hoe like that üíØ                                                 \n",
       "1743   <mention> \" nigger \"                                                                                                                            \n",
       "19811  youre a fucking ugly bitch , i wanna stab you to death and then play around with your blood - patrick bateman                                   \n",
       "18137  these niggas retarded ... lmfaoo                                                                                                                \n",
       "4526   bullets through a nigguh tru jeans                                                                                                              \n",
       "16180  rt <mention> : who wants to chill on campus w me & throw bibles at bitches taking the walk of shame ‚ùì ‚ùî                                         \n",
       "3274   <mention> ur ballsack is hairy gtfo tranny prude bitch                                                                                          \n",
       "3269   <mention> im going to murder that fucking cunt                                                                                                  \n",
       "19825  zimmerman we comin for yo life bitch . http://t.co/khdz1vdnsk                                                                                   \n",
       "1854   <mention> your 2 . and 3 . nigga them hoes suck wayans brothers are not funny at all jamie foxx should be number 2                              \n",
       "12308  rt <mention> : nigga wanna bitch about a party y'all know my address knock at my door                                                           \n",
       "10498  rt <mention> : i hate trailer trash üò∑                                                                                                           \n",
       "4177   ayy carlton is a ghetto school now #switchedatbirth                                                                                             \n",
       "16770  raunchy ass hoe .                                                                                                                               \n",
       "10664  rt <mention> : anybody who trashes their own neighborhood after a perceived injustice is a fucking moron . #ferguson                            \n",
       "423    <mention> i don't follow you because everything about your company is white trash .                                                             \n",
       "14976  rt <mention> : in the future , gay culture will be considered cool by america's teens and instead of wiggers there will be straggots .          \n",
       "20223  hoe ass nigga üòπ                                                                                                                                 \n",
       "425    <mention> stop trolling me u creepy ass cracker ! ! !                                                                                           \n",
       "\n",
       "       hate  offensive  neither  \n",
       "19933  1     0          0        \n",
       "10717  1     0          0        \n",
       "12006  1     0          0        \n",
       "738    1     0          0        \n",
       "3340   1     0          0        \n",
       "13655  1     0          0        \n",
       "14387  1     0          0        \n",
       "2811   1     0          0        \n",
       "3255   1     0          0        \n",
       "4865   1     0          0        \n",
       "16574  1     0          0        \n",
       "11945  1     0          0        \n",
       "1743   1     0          0        \n",
       "19811  1     0          0        \n",
       "18137  1     0          0        \n",
       "4526   1     0          0        \n",
       "16180  1     0          0        \n",
       "3274   1     0          0        \n",
       "3269   1     0          0        \n",
       "19825  1     0          0        \n",
       "1854   1     0          0        \n",
       "12308  1     0          0        \n",
       "10498  1     0          0        \n",
       "4177   1     0          0        \n",
       "16770  1     0          0        \n",
       "10664  1     0          0        \n",
       "423    1     0          0        \n",
       "14976  1     0          0        \n",
       "20223  1     0          0        \n",
       "425    1     0          0        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"False negatives:\")\n",
    "display(data['train'].loc[false_neg, ['tweet_processed', 'hate', 'offensive', 'neither']].sample(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis (offensive language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train on full dataset and see errors\n",
    "clf = LogisticRegression()\n",
    "y['train'] = data['train'].offensive\n",
    "clf.fit(x['train'], y['train'])\n",
    "preds = clf.predict(x['train'])\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633\n",
      "477\n"
     ]
    }
   ],
   "source": [
    "false_pos = []\n",
    "false_neg = []\n",
    "\n",
    "for i, (gold, pred) in enumerate(zip(data['train'].offensive.tolist(), preds)):\n",
    "    if pred != gold:\n",
    "        if gold == 0:\n",
    "            false_pos.append(i)\n",
    "        elif gold == 1:\n",
    "            false_neg.append(i)\n",
    "        else:\n",
    "            \"What???\"\n",
    "            \n",
    "print(len(false_pos))\n",
    "print(len(false_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negatives:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_processed</th>\n",
       "      <th>hate</th>\n",
       "      <th>offensive</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16690</th>\n",
       "      <td>rt &lt;mention&gt; : &lt;mention&gt; blaxican</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15079</th>\n",
       "      <td>rt &lt;mention&gt; : üòÇ üòÇ rt &lt;mention&gt; : lol rt &lt;mention&gt; : üòÇ üò≠ rt &lt;mention&gt; : just so u faggots know ! http://t.co/bghtsjipne</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15728</th>\n",
       "      <td>rt &lt;mention&gt; : bruh you trash take yo ass back to college &lt;mention&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6141</th>\n",
       "      <td>hmm .. ass all chunky , brain is insanity .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12632</th>\n",
       "      <td>rt &lt;mention&gt; : i don't understand how so many americans can hate on obama for saluting with a latte . nobody tells these swagfags anything abo ‚Ä¶</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12728</th>\n",
       "      <td>rt &lt;mention&gt; : one man's trash is another man's girlfriend .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6373</th>\n",
       "      <td>i been way to consistent on niggas , i'm have to drop some trash to get appreciated</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15955</th>\n",
       "      <td>rt &lt;mention&gt; : i believe the time for uncouth / uncivilised people to be sentenced to death for racial slurs / attacks against ne citizens ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11039</th>\n",
       "      <td>rt &lt;mention&gt; : if you don't make her go to sleep after y'all fucked then your pipe game trash</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17480</th>\n",
       "      <td>stop spoiling u fags</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>gov . rick scott kicks charlie crist ‚Äô s ass http://t.co/yasz6yxfwc http://t.co/rdmcnzygbe</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>&lt;mention&gt; &lt;mention&gt; the only place for a ho in the garden</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>chocolate cake is so trash .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>&lt;mention&gt; &lt;mention&gt; jessica i'm not afraid to slit your throat tomorrow</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>almost everyone i met in #paris was absolute trash . and if they support a republican government , i hope they fucking die .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>&lt;mention&gt; when you realize you're trash</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7850</th>\n",
       "      <td>if luke hemmings really thinks they're trash , i must be fucking dirt bc you boys are beautiful dammit</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17509</th>\n",
       "      <td>stupid people talking smack on \" uncle tim tom \" are the same ghetto fools w / no future . i'm surprised u ignorant niggaz even read or write .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12701</th>\n",
       "      <td>rt &lt;mention&gt; : win % since pile of trash &lt;mention&gt; went to new orleans : #buffalo : 1.000 #neworleans : 0.0000000000 000 #fuckbyrd #whone ‚Ä¶</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>&lt;mention&gt; &lt;mention&gt; that nigga vic acting like he yellow bone with 15k followers and can't answer a nigga phone</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21078</th>\n",
       "      <td>you's a muthaf * * * in lie ‚Äú &lt;mention&gt; : &lt;mention&gt; &lt;mention&gt; right ! his tl is trash ‚Ä¶ . now , mine ? bible scriptures and hymns ‚Äù</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17573</th>\n",
       "      <td>take that and shove it up your ass , &lt;mention&gt; #yankees #derekjeter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14975</th>\n",
       "      <td>rt &lt;mention&gt; : charlie sheen is the realest nigga alive http://t.co/pk0nejiovh</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>&lt;mention&gt; ok faggot http://t.co/zdgte9gvzx</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>&lt;mention&gt; the bird gone run from your ass</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9228</th>\n",
       "      <td>murda mook ! ! !</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>&lt;mention&gt; you just can't give hillbillies an audience . they'll squander it every time .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>just dropped dat fine ass woman &lt;mention&gt; . we has da best chicken eva . i spectin some lovin but she a lady and da colored thang</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>&lt;mention&gt; but a wannabe ghetto white girl will deal with an unaccomplished black man for more than sex .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13875</th>\n",
       "      <td>rt &lt;mention&gt; : thanks for ignoring me faggots . i'm locked outside &lt;mention&gt; &lt;mention&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        tweet_processed  \\\n",
       "16690  rt <mention> : <mention> blaxican                                                                                                                  \n",
       "15079  rt <mention> : üòÇ üòÇ rt <mention> : lol rt <mention> : üòÇ üò≠ rt <mention> : just so u faggots know ! http://t.co/bghtsjipne                            \n",
       "15728  rt <mention> : bruh you trash take yo ass back to college <mention>                                                                                \n",
       "6141   hmm .. ass all chunky , brain is insanity .                                                                                                        \n",
       "12632  rt <mention> : i don't understand how so many americans can hate on obama for saluting with a latte . nobody tells these swagfags anything abo ‚Ä¶   \n",
       "12728  rt <mention> : one man's trash is another man's girlfriend .                                                                                       \n",
       "6373   i been way to consistent on niggas , i'm have to drop some trash to get appreciated                                                                \n",
       "15955  rt <mention> : i believe the time for uncouth / uncivilised people to be sentenced to death for racial slurs / attacks against ne citizens ...     \n",
       "11039  rt <mention> : if you don't make her go to sleep after y'all fucked then your pipe game trash                                                      \n",
       "17480  stop spoiling u fags                                                                                                                               \n",
       "5866   gov . rick scott kicks charlie crist ‚Äô s ass http://t.co/yasz6yxfwc http://t.co/rdmcnzygbe                                                         \n",
       "2349   <mention> <mention> the only place for a ho in the garden                                                                                          \n",
       "4771   chocolate cake is so trash .                                                                                                                       \n",
       "2561   <mention> <mention> jessica i'm not afraid to slit your throat tomorrow                                                                            \n",
       "3971   almost everyone i met in #paris was absolute trash . and if they support a republican government , i hope they fucking die .                       \n",
       "2142   <mention> when you realize you're trash                                                                                                            \n",
       "7850   if luke hemmings really thinks they're trash , i must be fucking dirt bc you boys are beautiful dammit                                             \n",
       "17509  stupid people talking smack on \" uncle tim tom \" are the same ghetto fools w / no future . i'm surprised u ignorant niggaz even read or write .    \n",
       "12701  rt <mention> : win % since pile of trash <mention> went to new orleans : #buffalo : 1.000 #neworleans : 0.0000000000 000 #fuckbyrd #whone ‚Ä¶        \n",
       "3258   <mention> <mention> that nigga vic acting like he yellow bone with 15k followers and can't answer a nigga phone                                    \n",
       "21078  you's a muthaf * * * in lie ‚Äú <mention> : <mention> <mention> right ! his tl is trash ‚Ä¶ . now , mine ? bible scriptures and hymns ‚Äù                \n",
       "17573  take that and shove it up your ass , <mention> #yankees #derekjeter                                                                                \n",
       "14975  rt <mention> : charlie sheen is the realest nigga alive http://t.co/pk0nejiovh                                                                     \n",
       "2341   <mention> ok faggot http://t.co/zdgte9gvzx                                                                                                         \n",
       "1618   <mention> the bird gone run from your ass                                                                                                          \n",
       "9228   murda mook ! ! !                                                                                                                                   \n",
       "1387   <mention> you just can't give hillbillies an audience . they'll squander it every time .                                                           \n",
       "8424   just dropped dat fine ass woman <mention> . we has da best chicken eva . i spectin some lovin but she a lady and da colored thang                  \n",
       "1145   <mention> but a wannabe ghetto white girl will deal with an unaccomplished black man for more than sex .                                           \n",
       "13875  rt <mention> : thanks for ignoring me faggots . i'm locked outside <mention> <mention>                                                             \n",
       "\n",
       "       hate  offensive  neither  \n",
       "16690  0     1          0        \n",
       "15079  0     1          0        \n",
       "15728  0     1          0        \n",
       "6141   0     1          0        \n",
       "12632  0     1          0        \n",
       "12728  0     1          0        \n",
       "6373   0     1          0        \n",
       "15955  0     1          0        \n",
       "11039  0     1          0        \n",
       "17480  0     1          0        \n",
       "5866   0     1          0        \n",
       "2349   0     1          0        \n",
       "4771   0     1          0        \n",
       "2561   0     1          0        \n",
       "3971   0     1          0        \n",
       "2142   0     1          0        \n",
       "7850   0     1          0        \n",
       "17509  0     1          0        \n",
       "12701  0     1          0        \n",
       "3258   0     1          0        \n",
       "21078  0     1          0        \n",
       "17573  0     1          0        \n",
       "14975  0     1          0        \n",
       "2341   0     1          0        \n",
       "1618   0     1          0        \n",
       "9228   0     1          0        \n",
       "1387   0     1          0        \n",
       "8424   0     1          0        \n",
       "1145   0     1          0        \n",
       "13875  0     1          0        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"False negatives:\")\n",
    "display(data['train'].loc[false_neg, ['tweet_processed', 'hate', 'offensive', 'neither']].sample(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21870 unique words\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 30000\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE,\n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n‚Äú‚Äù')\n",
    "texts = data['train']['tweet_processed'].tolist()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels = data['train']['hate'].data\n",
    "labels = to_categorical(data['train']['hate'].tolist())\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "seqs = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21870"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(word_index.keys())[:MAX_VOCAB_SIZE] # lower indices are words kept\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18975, 100)\n",
      "(18975, 2)\n",
      "(2108, 100)\n",
      "(2108, 2)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle, split into train/dev\n",
    "test_size = int(0.1 * len(seqs))\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(seqs, np.asarray(labels), test_size=test_size)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([[0.2, 0.3], [0.7, 0.3]])\n",
    "np.argmax(p, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "metrics(p, np.array([1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(preds, actual):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    matches = 0\n",
    "    \n",
    "    binary_preds = np.argmax(preds, axis=1)\n",
    "    binary_actual = np.argmax(actual, axis=1)\n",
    "    \n",
    "    for pred, act in zip(binary_preds, binary_actual):\n",
    "        \n",
    "        if pred == act == 1:\n",
    "            tp += 1\n",
    "            matches += 1\n",
    "        elif pred == 1 and act == 0:\n",
    "            fp += 1\n",
    "        elif pred == 0 and act == 1:\n",
    "            fn += 1\n",
    "        elif pred == act == 0:\n",
    "            matches += 1\n",
    "        \n",
    "        \n",
    "    if tp == fp == 0:\n",
    "        prec = 0\n",
    "    else:\n",
    "        prec = tp/(tp+fp)\n",
    "        \n",
    "    rec = tp/(tp+fn)\n",
    "    \n",
    "    if prec == rec == 0.0:\n",
    "        f1 = 0.0\n",
    "        \n",
    "    else:\n",
    "        f1 = 2 * prec * rec / (prec + rec)\n",
    "        \n",
    "    acc = matches/len(preds)\n",
    "    \n",
    "    print(f\"Precision: {prec}\")\n",
    "    print(f\"Recall: {rec}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "    print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18975 samples, validate on 2108 samples\n",
      "Epoch 1/50\n",
      "18975/18975 [==============================] - 6s 335us/step - loss: 0.2253 - val_loss: 0.2392\n",
      "Epoch 2/50\n",
      "18975/18975 [==============================] - 6s 300us/step - loss: 0.2199 - val_loss: 0.2410\n",
      "Epoch 3/50\n",
      "18975/18975 [==============================] - 5s 278us/step - loss: 0.2197 - val_loss: 0.2401\n",
      "\n",
      "Precision: 0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "Accuracy: 0.9354838709677419\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "embedding_layer = Embedding(len(vocab),\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length = MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True\n",
    "                           )\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(32, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(16, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='relu')) # final classification layer\n",
    "model.add(Dense(2, activation='softmax')) # final classification layer\n",
    "\n",
    "# adam = optimizers.adam(lr=0.1)\n",
    "sgd = optimizers.SGD(lr=0.05)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "#          batch_size=16, epochs=20, validation_data=(x_dev, y_dev))\n",
    "         batch_size=16, epochs=50, validation_data=(x_dev, y_dev), callbacks=callbacks)\n",
    "\n",
    "preds = model.predict(x_dev, batch_size=16)\n",
    "\n",
    "print()\n",
    "metrics(preds, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hatebase lexicon attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Hatebase lexicon\n",
    "# with open('/usr0/home/mamille2/11-830_data/project/hatebase_slurs.txt', 'r') as f:\n",
    "with open('../../project/hatebase_slurs.txt', 'r') as f:\n",
    "    slurs = set([w.lower() for w in f.read().splitlines()])\n",
    "    \n",
    "len(slurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: num_slurs, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Hatebase features\n",
    "\n",
    "data['train']['num_slurs'] = data['train']['tweet_processed'].map(lambda x: sum([1 for wd in x.split() if wd in slurs]))\n",
    "data['train']['num_slurs'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['num_slurs'].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 8534)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(min_df=2)\n",
    "vec.fit(data['train'].tweet_processed.tolist())\n",
    "\n",
    "bow = {}\n",
    "bow['train'] = vec.transform(data['train'].tweet_processed.tolist())\n",
    "bow['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.8%\n",
      "Precision:  48.0%\n",
      "Recall:  21.4%\n",
      "F1:  25.8%\n"
     ]
    }
   ],
   "source": [
    "# Hate speech classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], csr_matrix(data['train']['num_slurs'].data).T])\n",
    "y['train'] = data['train'].hate\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  89.1%\n",
      "Precision:  92.7%\n",
      "Recall:  93.2%\n",
      "F1:  93.0%\n"
     ]
    }
   ],
   "source": [
    "# Offensive language classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], csr_matrix(data['train']['num_slurs'].data).T])\n",
    "y['train'] = data['train'].offensive\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams (with hate speech lexicon pairing and not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 34274)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normal bigrams\n",
    "\n",
    "vec = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
    "vec.fit(data['train'].tweet_processed.tolist())\n",
    "\n",
    "bow = {}\n",
    "bow['train'] = vec.transform(data['train'].tweet_processed.tolist())\n",
    "bow['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.7%\n",
      "Precision:  52.9%\n",
      "Recall:  22.7%\n",
      "F1:  25.1%\n"
     ]
    }
   ],
   "source": [
    "# Hate speech classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = bow['train']\n",
    "y['train'] = data['train'].hate\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  89.4%\n",
      "Precision:  93.3%\n",
      "Recall:  93.1%\n",
      "F1:  93.1%\n"
     ]
    }
   ],
   "source": [
    "# Offensive language classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = bow['train']\n",
    "y['train'] = data['train'].offensive\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hate_pairs(tweet):\n",
    "    toks = tweet.split()\n",
    "    hate_dict = defaultdict(int)\n",
    "    \n",
    "    hate_terms = [w for w in toks if w in slurs]\n",
    "    for h in hate_terms:\n",
    "        for t in toks:\n",
    "            hate_dict[f'{h}_{t}'] += 1\n",
    "        \n",
    "    return hate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'nigger_<mention>': 2, 'nigger_they': 2, 'nigger_want': 3, 'nigger_what': 2, 'nigger_i': 3, 'nigger_got': 3, 'nigger_,': 3, 'nigger_america.they': 1, 'nigger_the': 1, 'nigger_word': 1, 'nigger_nigger': 1, 'nigger_it': 1, 'nigger_to': 1}                                                                                                                                                                                                                                               \n",
       "1    {}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "2    {'faggot_<mention>': 1, 'faggot_sion': 1, 'faggot_is': 1, 'faggot_a': 2, 'faggot_faggot': 1, 'faggot_.': 1, 'faggot_very': 1, 'faggot_meh': 2, 'faggot_ganks': 2, 'faggot_pre': 1, 'faggot_-': 2, 'faggot_6': 2, 'faggot_and': 1, 'faggot_post': 1, 'faggot_his': 1, 'faggot_are': 1, 'faggot_still': 1, 'faggot_pretty': 1, 'faggot_he's': 1, 'faggot_just': 1, 'faggot_tank': 1, 'faggot_,': 1, 'faggot_nothing': 1, 'faggot_special': 1, 'faggot_imo': 1, 'faggot_mao': 1, 'faggot_+': 1}\n",
       "3    {'redneck_<mention>': 3, 'redneck_your': 1, 'redneck_low': 1, 'redneck_info': 1, 'redneck_redneck': 1, 'redneck_dumbasses': 1, 'redneck_vote': 1, 'redneck_for': 1, 'redneck_these': 1, 'redneck_corporate': 1, 'redneck_shills': 1, 'redneck_that': 1, 'redneck_idiots': 1, 'redneck_like': 1, 'redneck_you': 1, 'redneck_believe': 1, 'redneck_#dummies': 1}                                                                                                                              \n",
       "4    {}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "5    {}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "6    {}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "7    {'whitey_<mention>': 1, 'whitey_ok': 1, 'whitey_wat': 1, 'whitey_ever': 1, 'whitey_u': 2, 'whitey_say': 1, 'whitey_whitey': 1, 'whitey_.': 2, 'whitey_prolly': 1, 'whitey_never': 1, 'whitey_even': 1, 'whitey_seen': 1, 'whitey_shaft': 1}                                                                                                                                                                                                                                                 \n",
       "8    {'faggot_<mention>': 1, 'faggot_shut': 1, 'faggot_up': 1, 'faggot_lizard': 1, 'faggot_faggot': 1, 'faggot_nigger': 1, 'faggot_cunt': 1, 'nigger_<mention>': 1, 'nigger_shut': 1, 'nigger_up': 1, 'nigger_lizard': 1, 'nigger_faggot': 1, 'nigger_nigger': 1, 'nigger_cunt': 1, 'cunt_<mention>': 1, 'cunt_shut': 1, 'cunt_up': 1, 'cunt_lizard': 1, 'cunt_faggot': 1, 'cunt_nigger': 1, 'cunt_cunt': 1}                                                                                     \n",
       "9    {}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "Name: tweet_processed, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract dict features\n",
    "hate_pairs = data['train']['tweet_processed'].map(get_hate_pairs)\n",
    "hate_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 57450)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build special pairings with hatebase terms\n",
    "dict_vec = DictVectorizer()\n",
    "\n",
    "hate_feats = dict_vec.fit_transform(hate_pairs)\n",
    "hate_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 8534)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(min_df=2)\n",
    "vec.fit(data['train'].tweet_processed.tolist())\n",
    "\n",
    "bow = {}\n",
    "bow['train'] = vec.transform(data['train'].tweet_processed.tolist())\n",
    "bow['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 65984)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.5%\n",
      "Precision:  47.1%\n",
      "Recall:  22.2%\n",
      "F1:  25.2%\n"
     ]
    }
   ],
   "source": [
    "# Hate speech classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], hate_feats])\n",
    "y['train'] = data['train'].hate\n",
    "\n",
    "# clf = make_pipeline(SelectKBest(chi2, k=30000), LogisticRegression())\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  90.0%\n",
      "Precision:  93.7%\n",
      "Recall:  93.4%\n",
      "F1:  93.5%\n"
     ]
    }
   ],
   "source": [
    "# Offensive language classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], hate_feats])\n",
    "y['train'] = data['train'].offensive\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violence words (doesn't help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viol_wds = ['kill', 'murder', 'stab', 'death', 'bullets', 'blood', 'knife', 'gun', 'shoot', 'life', 'beat', 'hate',\n",
    "           'fight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_viol_pairs(tweet):\n",
    "    toks = tweet.split()\n",
    "    hate_dict = defaultdict(int)\n",
    "    \n",
    "    hate_terms = [w for w in toks if w in viol_wds]\n",
    "    for h in hate_terms:\n",
    "        for t in toks:\n",
    "            hate_dict[f'{h}_{t}'] += 1\n",
    "        \n",
    "    return hate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {}\n",
       "1    {}\n",
       "2    {}\n",
       "3    {}\n",
       "4    {}\n",
       "5    {}\n",
       "6    {}\n",
       "7    {}\n",
       "8    {}\n",
       "9    {}\n",
       "Name: tweet_processed, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract dict features\n",
    "hate_pairs = data['train']['tweet_processed'].map(get_viol_pairs)\n",
    "hate_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1094"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([h for h in hate_pairs.tolist() if len(h) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 6339)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build special pairings with hatebase terms\n",
    "dict_vec = DictVectorizer()\n",
    "\n",
    "hate_feats = dict_vec.fit_transform(hate_pairs)\n",
    "hate_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 8534)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(min_df=2)\n",
    "vec.fit(data['train'].tweet_processed.tolist())\n",
    "\n",
    "bow = {}\n",
    "bow['train'] = vec.transform(data['train'].tweet_processed.tolist())\n",
    "bow['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.7%\n",
      "Precision:  46.6%\n",
      "Recall:  21.0%\n",
      "F1:  25.3%\n"
     ]
    }
   ],
   "source": [
    "# Hate speech classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], hate_feats])\n",
    "# x['train'] = bow['train']\n",
    "y['train'] = data['train'].hate\n",
    "\n",
    "clf = make_pipeline(SelectKBest(chi2, k=10000), LogisticRegression())\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  89.1%\n",
      "Precision:  92.8%\n",
      "Recall:  93.2%\n",
      "F1:  93.0%\n"
     ]
    }
   ],
   "source": [
    "# Offensive language classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], hate_feats])\n",
    "y['train'] = data['train'].offensive\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NNS VBP PRP VBP WP NN VBD , VB VBP WP PRP VBD ...\n",
       "1                    JJ NN NN NN FW RB IN DT NNS VBP .\n",
       "2    JJ NN VBZ DT NN . RB JJ NNS VBP : CD CC VB : C...\n",
       "3    JJ NNP NNP PRP$ JJ NN NN NNS VBP IN DT JJ NNS ...\n",
       "4                            RB VB IN IN PRP$ NN NNS .\n",
       "Name: pos, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['pos'] = data['train']['tweet_processed'].map(lambda x: ' '.join([pos for wd, pos in pos_tag(x.split())]))\n",
    "data['train']['pos'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_contexts(text, postags, window_size):\n",
    "    toks = text.split()\n",
    "    tags = postags.split()\n",
    "    feats = []\n",
    "    \n",
    "    assert len(toks) == len(tags)\n",
    "    \n",
    "    for i in range(len(toks)):\n",
    "        if i == 0:\n",
    "            feats.append('_'.join([toks[i]] + tags[i+1:i+1+window_size]))\n",
    "        elif i == len(toks) - 1:\n",
    "            feats.append('_'.join(tags[i-window_size:i] + [tags[i]]))\n",
    "        else:\n",
    "            feats.append('_'.join(tags[i-window_size:i] + [toks[i]] + tags[i+1:i+1+window_size]))\n",
    "    \n",
    "    return Counter(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21083"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_templates = [pos_contexts(t, postags, 1) for (t, postags) in list(zip(data['train']['tweet_processed'], data['train']['pos']))]\n",
    "len(pos_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 106507)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build special pairings with hatebase terms\n",
    "dict_vec = DictVectorizer()\n",
    "\n",
    "hate_feats = dict_vec.fit_transform(pos_templates)\n",
    "hate_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 8534)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(min_df=2)\n",
    "vec.fit(data['train'].tweet_processed.tolist())\n",
    "\n",
    "bow = {}\n",
    "bow['train'] = vec.transform(data['train'].tweet_processed.tolist())\n",
    "bow['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  89.9%\n",
      "Precision:  43.8%\n",
      "Recall:  19.0%\n",
      "F1:  13.4%\n"
     ]
    }
   ],
   "source": [
    "# Hate speech classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], hate_feats])\n",
    "y['train'] = data['train'].hate\n",
    "\n",
    "clf = make_pipeline(SelectKBest(chi2, k=10000), LogisticRegression())\n",
    "# clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  88.3%\n",
      "Precision:  93.3%\n",
      "Recall:  91.6%\n",
      "F1:  92.3%\n"
     ]
    }
   ],
   "source": [
    "# Offensive language classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], hate_feats])\n",
    "y['train'] = data['train'].offensive\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS ngrams (doesn't improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NNS VBP PRP VBP WP NN VBD , VB VBP WP PRP VBD , NN VBD DT NN NN , NN VBP PRP TO                   \n",
       "1    JJ NN NN NN FW RB IN DT NNS VBP .                                                                 \n",
       "2    JJ NN VBZ DT NN . RB JJ NNS VBP : CD CC VB : CD PRP$ NNS VBP RB RB JJ NN RB DT NN , NN JJ NN NN NN\n",
       "3    JJ NNP NNP PRP$ JJ NN NN NNS VBP IN DT JJ NNS IN NNS IN PRP VBP NNS                               \n",
       "4    RB VB IN IN PRP$ NN NNS .                                                                         \n",
       "Name: pos, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['pos'] = data['train']['tweet_processed'].map(lambda x: ' '.join([pos for wd, pos in pos_tag(x.split())]))\n",
    "data['train']['pos'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21083, 8972)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract ngrams\n",
    "pos_vec = CountVectorizer(ngram_range=(1,3))\n",
    "pos_feats = pos_vec.fit_transform(data['train']['pos'])\n",
    "pos_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.0%\n",
      "Precision:  41.9%\n",
      "Recall:  22.5%\n",
      "F1:  24.5%\n"
     ]
    }
   ],
   "source": [
    "# Hate speech classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], pos_feats])\n",
    "y['train'] = data['train'].hate\n",
    "\n",
    "# clf = make_pipeline(SelectKBest(chi2, k=10000), LogisticRegression())\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  87.9%\n",
      "Precision:  92.0%\n",
      "Recall:  92.4%\n",
      "F1:  92.2%\n"
     ]
    }
   ],
   "source": [
    "# Offensive language classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], pos_feats])\n",
    "y['train'] = data['train'].offensive\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hate density (doesn't improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slurs_norm(num_slurs, text):\n",
    "    return num_slurs/len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.041667\n",
       "1    0.000000\n",
       "2    0.031250\n",
       "3    0.052632\n",
       "4    0.000000\n",
       "Name: slurs_normalized, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['slurs_normalized'] = [slurs_norm(ns, t) for (ns, t) in list(zip(data['train']['num_slurs'], data['train']['tweet_processed']))]\n",
    "data['train']['slurs_normalized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  93.8%\n",
      "Precision:  48.1%\n",
      "Recall:  21.1%\n",
      "F1:  25.6%\n"
     ]
    }
   ],
   "source": [
    "# Hate speech classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], csr_matrix(data['train']['slurs_normalized'].data).T])\n",
    "y['train'] = data['train'].hate\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  89.2%\n",
      "Precision:  92.8%\n",
      "Recall:  93.3%\n",
      "F1:  93.0%\n"
     ]
    }
   ],
   "source": [
    "# Offensive language classification\n",
    "x = {}\n",
    "y = {}\n",
    "x['train'] = hstack([bow['train'], csr_matrix(data['train']['slurs_normalized'].data).T])\n",
    "y['train'] = data['train'].offensive\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "scoring = {'accuracy': make_scorer(accuracy_score), \n",
    "           'precision': make_scorer(precision_score),\n",
    "          'recall': make_scorer(recall_score),\n",
    "          'f1': make_scorer(f1_score)}\n",
    "\n",
    "scores = cross_validate(clf, x['train'], y['train'], cv=5, scoring=scoring)\n",
    "print(f'Accuracy: {scores[\"test_accuracy\"].mean(): .1%}')\n",
    "print(f'Precision: {scores[\"test_precision\"].mean(): .1%}')\n",
    "print(f'Recall: {scores[\"test_recall\"].mean(): .1%}')\n",
    "print(f'F1: {scores[\"test_f1\"].mean(): .1%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
